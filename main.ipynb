{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ef72f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pysoundfile\n",
      "  Downloading PySoundFile-0.9.0.post1-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: cffi>=0.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pysoundfile) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from cffi>=0.6->pysoundfile) (2.20)\n",
      "Installing collected packages: pysoundfile\n",
      "Successfully installed pysoundfile-0.9.0.post1\n"
     ]
    }
   ],
   "source": [
    "# jupyter安装soundfile 新开terminal输入\n",
    "'''\n",
    "sudo yum update -y\n",
    "# sudo wget http://www.mega-nerd.com/libsndfile/files/libsndfile-1.0.28.tar.gz\n",
    "# tar -xzf libsndfile-1.0.28.tar.gz\n",
    "cd /home/ec2-user/SageMaker/asteroid_byoc/\n",
    "cd libsndfile-1.0.28\n",
    "./configure --prefix=/usr --disable-static --docdir=/usr/share/doc/libsndfile-1.0.28 && make\n",
    "sudo make install\n",
    "'''\n",
    "\n",
    "# 构建镜像，确认权限\n",
    "'''\n",
    "chmod a+x build_and_push.sh\n",
    "./build_and_push.sh asteroid\n",
    "/bin/bash ./helper/setup.sh\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "dc996b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.5.0-cpu-py36-ubuntu16.04\n",
      "\n",
      "RUN apt-get update \n",
      "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\n",
      "         nginx \\\n",
      "         ca-certificates \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "    \n",
      "RUN wget http://www.mega-nerd.com/libsndfile/files/libsndfile-1.0.28.tar.gz\n",
      "RUN tar -xzf libsndfile-1.0.28.tar.gz\n",
      "RUN cd libsndfile-1.0.28 && ./configure --prefix=/usr --disable-static --docdir=/usr/share/doc/libsndfile-1.0.28 && make && make install\n",
      "RUN pip install pysoundfile numpy numba Cython asteroid sagemaker-inference PyYAML pytorch-lightning==1.4.9 flask gunicorn\n",
      "\n",
      "ENV PATH=\"/opt/ml/code:${PATH}\"\n",
      "\n",
      "# /opt/ml and all subdirectories are utilized by SageMaker, we use the /code subdirectory to store our user code.\n",
      "ENV PYTHONUNBUFFERED=TRUE\n",
      "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      "\n",
      "COPY ./ /opt/ml/code\n",
      "WORKDIR /opt/ml/code\n"
     ]
    }
   ],
   "source": [
    "!cat ./code/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "551200d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.9.0-py3-none-any.whl (211 kB)\n",
      "     |████████████████████████████████| 211 kB 14.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (21.3)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (1.0.1)\n",
      "Collecting resampy>=0.2.2\n",
      "  Downloading resampy-0.2.2.tar.gz (323 kB)\n",
      "     |████████████████████████████████| 323 kB 53.1 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pooch>=1.0\n",
      "  Downloading pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "     |████████████████████████████████| 56 kB 4.3 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (1.5.3)\n",
      "Requirement already satisfied: numba>=0.45.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (0.53.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (1.19.5)\n",
      "Collecting soundfile>=0.10.2\n",
      "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
      "Collecting audioread>=2.1.5\n",
      "  Downloading audioread-2.1.9.tar.gz (377 kB)\n",
      "     |████████████████████████████████| 377 kB 45.5 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (5.0.9)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (0.24.2)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from numba>=0.45.1->librosa) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from numba>=0.45.1->librosa) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->librosa) (2.4.7)\n",
      "Collecting appdirs>=1.3.0\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pooch>=1.0->librosa) (2.25.1)\n",
      "Requirement already satisfied: six>=1.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from resampy>=0.2.2->librosa) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn>=0.19.1->librosa) (2.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from soundfile>=0.10.2->librosa) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.5)\n",
      "Building wheels for collected packages: audioread, resampy\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-2.1.9-py3-none-any.whl size=23308 sha256=1305dc71783842e5792d746a7574885b0f9f5e4bab59cae3786be05815fd0627\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/de/14/0a/863e4ed680b3204444cf486733e609d7ff7abe8fceafab67dc\n",
      "  Building wheel for resampy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=321179 sha256=25fe9703e1fe6a5bd04f21f6a290f7159e72ad88231e0048ccd0905d3465a0d7\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/cf/d4/04/49d8824a42bd9f9b11d502727965b9997f0d41d2b22ae4f645\n",
      "Successfully built audioread resampy\n",
      "Installing collected packages: appdirs, soundfile, resampy, pooch, audioread, librosa\n",
      "Successfully installed appdirs-1.4.4 audioread-2.1.9 librosa-0.9.0 pooch-1.6.0 resampy-0.2.2 soundfile-0.10.3.post1\n",
      "Collecting asteroid\n",
      "  Downloading asteroid-0.5.2-py3-none-any.whl (246 kB)\n",
      "     |████████████████████████████████| 246 kB 20.0 MB/s            \n",
      "\u001b[?25hCollecting asteroid-filterbanks>=0.4.0\n",
      "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: SoundFile>=0.10.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from asteroid) (0.10.3.post1)\n",
      "Collecting pb-bss-eval>=0.0.2\n",
      "  Downloading pb_bss_eval-0.0.2-py3-none-any.whl (14 kB)\n",
      "Collecting torch-optimizer<0.2.0,>=0.0.1a12\n",
      "  Downloading torch_optimizer-0.1.0-py3-none-any.whl (72 kB)\n",
      "     |████████████████████████████████| 72 kB 2.4 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from asteroid) (5.4.1)\n",
      "Requirement already satisfied: pandas>=0.23.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from asteroid) (1.1.5)\n",
      "Collecting torch-stoi>=0.1.2\n",
      "  Downloading torch_stoi-0.1.2.tar.gz (6.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torchaudio>=0.5.0\n",
      "  Downloading torchaudio-0.10.1-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "     |████████████████████████████████| 2.9 MB 73.1 MB/s            \n",
      "\u001b[?25hCollecting pytorch-lightning<1.5.0,>=1.0.1\n",
      "  Downloading pytorch_lightning-1.4.9-py3-none-any.whl (925 kB)\n",
      "     |████████████████████████████████| 925 kB 80.4 MB/s            \n",
      "\u001b[?25hCollecting julius\n",
      "  Downloading julius-0.2.6.tar.gz (58 kB)\n",
      "     |████████████████████████████████| 58 kB 8.7 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from asteroid) (1.19.5)\n",
      "Collecting torch>=1.8.0\n",
      "  Downloading torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "     |████████████████████████████████| 881.9 MB 5.4 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from asteroid) (1.5.3)\n",
      "Collecting huggingface-hub>=0.0.2\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "     |████████████████████████████████| 67 kB 11.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from asteroid-filterbanks>=0.4.0->asteroid) (3.10.0.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from huggingface-hub>=0.0.2->asteroid) (4.61.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from huggingface-hub>=0.0.2->asteroid) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from huggingface-hub>=0.0.2->asteroid) (21.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from huggingface-hub>=0.0.2->asteroid) (2.25.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from huggingface-hub>=0.0.2->asteroid) (3.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas>=0.23.4->asteroid) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas>=0.23.4->asteroid) (2021.1)\n",
      "Collecting pystoi\n",
      "  Downloading pystoi-0.3.3.tar.gz (7.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting mir-eval\n",
      "  Downloading mir_eval-0.6.tar.gz (87 kB)\n",
      "     |████████████████████████████████| 87 kB 13.4 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pb-bss-eval>=0.0.2->asteroid) (1.5.2)\n",
      "Collecting einops\n",
      "  Downloading einops-0.4.0-py3-none-any.whl (28 kB)\n",
      "Collecting pesq\n",
      "  Downloading pesq-0.0.3.tar.gz (35 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
      "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
      "     |████████████████████████████████| 133 kB 10.9 MB/s            \n",
      "\u001b[?25hCollecting torchmetrics>=0.4.0\n",
      "  Downloading torchmetrics-0.7.2-py3-none-any.whl (397 kB)\n",
      "     |████████████████████████████████| 397 kB 77.2 MB/s            \n",
      "\u001b[?25hCollecting tensorboard>=2.2.0\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "     |████████████████████████████████| 5.8 MB 52.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: future>=0.17.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-lightning<1.5.0,>=1.0.1->asteroid) (0.18.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from SoundFile>=0.10.2->asteroid) (1.14.5)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=1.8.0->asteroid) (0.8)\n",
      "Collecting pytorch-ranger>=0.1.1\n",
      "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
      "Collecting torch>=1.8.0\n",
      "  Downloading torch-1.10.1-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "     |████████████████████████████████| 881.9 MB 4.7 kB/s             ████████████████████████▊ | 846.9 MB 67.6 MB/s eta 0:00:01 MB 67.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from cffi>=1.0->SoundFile>=0.10.2->asteroid) (2.20)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.9->huggingface-hub>=0.0.2->asteroid) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas>=0.23.4->asteroid) (1.16.0)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.43.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "     |████████████████████████████████| 4.1 MB 67.2 MB/s            \n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     |████████████████████████████████| 781 kB 68.0 MB/s            \n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 63.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (1.30.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (2.0.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (52.0.0.post20210125)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "     |████████████████████████████████| 126 kB 66.8 MB/s            \n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "     |████████████████████████████████| 97 kB 11.7 MB/s            \n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (0.36.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (3.17.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->huggingface-hub>=0.0.2->asteroid) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->huggingface-hub>=0.0.2->asteroid) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->huggingface-hub>=0.0.2->asteroid) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->huggingface-hub>=0.0.2->asteroid) (1.26.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->huggingface-hub>=0.0.2->asteroid) (3.4.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (4.2.2)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (2.0.9)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (4.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (1.2.0)\n",
      "Requirement already satisfied: idna-ssl>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (1.1.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (5.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (21.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.5.0,>=1.0.1->asteroid) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     |████████████████████████████████| 151 kB 81.8 MB/s            \n",
      "\u001b[?25hBuilding wheels for collected packages: torch-stoi, julius, mir-eval, pesq, pystoi\n",
      "  Building wheel for torch-stoi (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch-stoi: filename=torch_stoi-0.1.2-py3-none-any.whl size=6226 sha256=a5bdcb5179c7df19041665eae01bc552f42fefbbdda976fefbc905ac068dae5d\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c6/cd/e0/59776ada568467e1388441fa37bfe2fdd1d380a1dd7ee8b09d\n",
      "  Building wheel for julius (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for julius: filename=julius-0.2.6-py3-none-any.whl size=21131 sha256=2fcb7f7f0cee388ae7eb6060cdeafa18f854488af39d725ce88aa29ea4968231\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/d5/5c/42/762f6daf81d3ada114a9e432bb611fa0d191c2ab22f6c230ba\n",
      "  Building wheel for mir-eval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mir-eval: filename=mir_eval-0.6-py3-none-any.whl size=97207 sha256=19b1f1bdcd94860ad7aea3187a35e74a7caf9da73309dc8ae8f4a08e130dacf0\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/33/84/cf/b9da5f4e21a4d2ae3112774eb27ffd40fbd63cb68fc3195bf5\n",
      "  Building wheel for pesq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pesq: filename=pesq-0.0.3-cp36-cp36m-linux_x86_64.whl size=111117 sha256=8ff4e742a0e551a835867022c6e937fc71975d1ddcb5db77cb8e320a3eced845\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/b2/94/37/87ba1f402a9a10b6661470ca6b8251fd77df95efddffa4895e\n",
      "  Building wheel for pystoi (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pystoi: filename=pystoi-0.3.3-py2.py3-none-any.whl size=7827 sha256=ebbcc8a97b308abd75f61b3601f73ecb396dbab9851b903b7ddb5a0ecb1f0704\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/47/80/53/bef5a8298048bc5d90f4460f3db05f6b53b1dff4cfa90349bb\n",
      "Successfully built torch-stoi julius mir-eval pesq pystoi\n",
      "Installing collected packages: oauthlib, requests-oauthlib, torch, tensorboard-plugin-wit, tensorboard-data-server, pyDeprecate, markdown, grpcio, google-auth-oauthlib, fsspec, absl-py, torchmetrics, torchaudio, tensorboard, pytorch-ranger, pystoi, pesq, mir-eval, einops, torch-stoi, torch-optimizer, pytorch-lightning, pb-bss-eval, julius, huggingface-hub, asteroid-filterbanks, asteroid\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.4.0\n",
      "    Uninstalling fsspec-2021.4.0:\n",
      "      Successfully uninstalled fsspec-2021.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\n",
      "s3fs 2021.4.0 requires fsspec==2021.04.0, but you have fsspec 2022.1.0 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-1.0.0 asteroid-0.5.2 asteroid-filterbanks-0.4.0 einops-0.4.0 fsspec-2022.1.0 google-auth-oauthlib-0.4.6 grpcio-1.43.0 huggingface-hub-0.4.0 julius-0.2.6 markdown-3.3.6 mir-eval-0.6 oauthlib-3.2.0 pb-bss-eval-0.0.2 pesq-0.0.3 pyDeprecate-0.3.1 pystoi-0.3.3 pytorch-lightning-1.4.9 pytorch-ranger-0.1.1 requests-oauthlib-1.3.1 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torch-1.10.1 torch-optimizer-0.1.0 torch-stoi-0.1.2 torchaudio-0.10.1 torchmetrics-0.7.2\n",
      "Collecting pysoundfile\n",
      "  Downloading PySoundFile-0.9.0.post1-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: cffi>=0.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pysoundfile) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from cffi>=0.6->pysoundfile) (2.20)\n",
      "Installing collected packages: pysoundfile\n",
      "Successfully installed pysoundfile-0.9.0.post1\n"
     ]
    }
   ],
   "source": [
    "#jupyter instance安装依赖项\n",
    "!pip install librosa\n",
    "!pip install asteroid \n",
    "!pip install pysoundfile\n",
    "\n",
    "import soundfile\n",
    "import librosa\n",
    "import asteroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30502d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sagemaker\n",
    "import torch\n",
    "from sagemaker import get_execution_role\n",
    "from helper.utils import *\n",
    "import argparse\n",
    "import librosa\n",
    "import torch\n",
    "from scipy.signal import lfilter\n",
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "import soundfile as sf\n",
    "import fnmatch, os, warnings\n",
    "import json\n",
    "from code.pred_utils import *\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "_role = get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sample_rate=16000 #采样率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63e2e9e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n",
      "310850127430.dkr.ecr.us-east-1.amazonaws.com/asteroid:latest\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  226.7MB\n",
      "Step 1/9 : FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.5.0-cpu-py36-ubuntu16.04\n",
      " ---> e77085e0f844\n",
      "Step 2/9 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 5c2fa9faa2f6\n",
      "Step 3/9 : RUN wget http://www.mega-nerd.com/libsndfile/files/libsndfile-1.0.28.tar.gz\n",
      " ---> Using cache\n",
      " ---> a16532ede4e1\n",
      "Step 4/9 : RUN tar -xzf libsndfile-1.0.28.tar.gz\n",
      " ---> Using cache\n",
      " ---> c7fb795b5c82\n",
      "Step 5/9 : RUN cd libsndfile-1.0.28 && ./configure --prefix=/usr --disable-static --docdir=/usr/share/doc/libsndfile-1.0.28 && make && make install\n",
      " ---> Using cache\n",
      " ---> 45d2145f0d2b\n",
      "Step 6/9 : RUN pip install pysoundfile numpy numba Cython asteroid sagemaker-inference PyYAML pytorch-lightning==1.4.9\n",
      " ---> Using cache\n",
      " ---> 45e264ec8d4f\n",
      "Step 7/9 : ENV PATH=\"/opt/ml/code:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> a0e4880ff501\n",
      "Step 8/9 : COPY ./ /opt/ml/code\n",
      " ---> 09dd8c1d34de\n",
      "Step 9/9 : WORKDIR /opt/ml/code\n",
      " ---> Running in 458499eb9288\n",
      "Removing intermediate container 458499eb9288\n",
      " ---> da8176747d7f\n",
      "Successfully built da8176747d7f\n",
      "Successfully tagged asteroid:latest\n",
      "The push refers to repository [310850127430.dkr.ecr.us-east-1.amazonaws.com/asteroid]\n",
      "\n",
      "\u001b[1B57393cf1: Preparing \n",
      "\u001b[1Ba3f8ec1d: Preparing \n",
      "\u001b[1B92282ec5: Preparing \n",
      "\u001b[1B337f4571: Preparing \n",
      "\u001b[1Bb1dba928: Preparing \n",
      "\u001b[1Bd3311b0d: Preparing \n",
      "\u001b[1Be55345f6: Preparing \n",
      "\u001b[1B7f99fee4: Preparing \n",
      "\u001b[1B4f416d27: Preparing \n",
      "\u001b[1B9194a89a: Preparing \n",
      "\u001b[1Bba7c32f9: Preparing \n",
      "\u001b[1Bf976152c: Preparing \n",
      "\u001b[1Ba4625f76: Preparing \n",
      "\u001b[1Bd7f45c93: Preparing \n",
      "\u001b[7B4f416d27: Waiting g \n",
      "\u001b[1Bf83539e8: Preparing \n",
      "\u001b[8B9194a89a: Waiting g \n",
      "\u001b[12B55345f6: Waiting g \n",
      "\u001b[12Bf99fee4: Waiting g \n",
      "\u001b[1B49baa658: Preparing \n",
      "\u001b[21B7393cf1: Pushed   226.7MB/225.8MB5A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2Klatest: digest: sha256:778615fee445f812a9ab3c80ac3b786b79ac8a5fb755094f8ba768377efa95e8 size: 4729\n",
      "Instance type = ml.p3.2xlarge\n",
      "2022-02-04 13:58:24 Starting - Starting the training job...\n",
      "2022-02-04 13:58:48 Starting - Launching requested ML instancesProfilerReport-1643983104: InProgress\n",
      "......\n",
      "2022-02-04 13:59:48 Starting - Preparing the instances for training.........\n",
      "2022-02-04 14:01:08 Downloading - Downloading input data\n",
      "2022-02-04 14:01:08 Training - Downloading the training image.....................\n",
      "2022-02-04 14:04:49 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34msed: can't read changehostname.c: No such file or directory\u001b[0m\n",
      "\u001b[34mgcc: error: changehostname.c: No such file or directory\u001b[0m\n",
      "\u001b[34mgcc: fatal error: no input files\u001b[0m\n",
      "\u001b[34mcompilation terminated.\u001b[0m\n",
      "\u001b[34mgcc: error: changehostname.o: No such file or directory\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mtrain_cmd: ['/opt/conda/bin/python', 'code/dprnn-train-deploy.py']\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mdef_conf {'data': {'mode': 'min', 'model_dir': '/opt/ml/model', 'nondefault_nsrc': None, 'sample_rate': 16000, 'segment': 1.0, 'task': 'sep_noisy', 'train_dir': '/opt/ml/input/data/training', 'valid_dir': '/opt/ml/input/data/training'}, 'filterbank': {'kernel_size': 2, 'n_filters': 64, 'stride': 1}, 'main_args': {'help': None}, 'masknet': {'bidirectional': True, 'bn_chan': 128, 'chunk_size': 250, 'dropout': 0, 'hid_size': 128, 'hop_size': 125, 'in_chan': 64, 'mask_act': 'sigmoid', 'n_repeats': 6, 'n_src': 2, 'out_chan': 64}, 'optim': {'lr': 0.05, 'optimizer': 'adam', 'weight_decay': 1e-05}, 'positional arguments': {}, 'training': {'batch_size': 2, 'early_stop': True, 'epochs': 1, 'gradient_clipping': 5, 'half_lr': True, 'num_workers': 1, 'use_cuda': False}}\u001b[0m\n",
      "\u001b[34mpp ArgumentParser(prog='dprnn-train-deploy.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='resolve', add_help=True)\u001b[0m\n",
      "\u001b[34mDrop 0 utts(0.00 h) from 62 (shorter than 16000 samples)\u001b[0m\n",
      "\u001b[34mtrain_set <wham_dataset_no_sf.WhamDataset_no_sf object at 0x7f6aca4526d8>\u001b[0m\n",
      "\u001b[34mDrop 0 utts(0.00 h) from 62 (shorter than 16000 samples)\u001b[0m\n",
      "\u001b[34mtrain_loader <torch.utils.data.dataloader.DataLoader object at 0x7f6aca452048>\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:111: LightningDeprecationWarning: `Trainer(distributed_backend=ddp)` has been deprecated and will be removed in v1.5. Use `Trainer(accelerator=ddp)` instead.\n",
      "  f\"`Trainer(distributed_backend={distributed_backend})` has been deprecated and will be removed in v1.5.\"\u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34minitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed_backend=nccl\u001b[0m\n",
      "\u001b[34mAll DDP processes registered. Starting ddp with 1 processes\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  | Name      | Type           | Params\u001b[0m\n",
      "\u001b[34m---------------------------------------------\u001b[0m\n",
      "\u001b[34m0 | model     | DPRNNTasNet    | 3.7 M \u001b[0m\n",
      "\u001b[34m1 | loss_func | PITLossWrapper | 0     \u001b[0m\n",
      "\u001b[34m---------------------------------------------\u001b[0m\n",
      "\u001b[34m3.7 M     Trainable params\u001b[0m\n",
      "\u001b[34m0         Non-trainable params\u001b[0m\n",
      "\u001b[34m3.7 M     Total params\u001b[0m\n",
      "\u001b[34m14.604    Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:327: UserWarning: The number of training samples (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m#015Validation sanity check: 0it [00:00, ?it/s]#015Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]#015Validation sanity check:  50%|█████     | 1/2 [00:00<00:00,  3.71it/s]#015Validation sanity check: 100%|██████████| 2/2 [00:00<00:00,  5.04it/s]#015                                                                      #015#015Training: -1it [00:00, ?it/s]#015Training:   0%|          | 0/62 [00:00<00:00, 21509.25it/s]#015Epoch 0:   0%|          | 0/62 [00:00<00:00, 4624.37it/s]  #015Epoch 0:   2%|▏         | 1/62 [00:00<00:14,  4.09it/s]  #015Epoch 0:   2%|▏         | 1/62 [00:00<00:14,  4.09it/s, loss=28, v_num=0]#015Epoch 0:   3%|▎         | 2/62 [00:00<00:18,  3.27it/s, loss=28, v_num=0]#015Epoch 0:   3%|▎         | 2/62 [00:00<00:18,  3.27it/s, loss=26, v_num=0]#015Epoch 0:   5%|▍         | 3/62 [00:01<00:19,  2.97it/s, loss=26, v_num=0]#015Epoch 0:   5%|▍         | 3/62 [00:01<00:19,  2.97it/s, loss=28.2, v_num=0]#015Epoch 0:   6%|▋         | 4/62 [00:01<00:20,  2.82it/s, loss=28.2, v_num=0]#015Epoch 0:   6%|▋         | 4/62 [00:01<00:20,  2.82it/s, loss=28.6, v_num=0]#015Epoch 0:   8%|▊         | 5/62 [00:02<00:20,  2.73it/s, loss=28.6, v_num=0]#015Epoch 0:   8%|▊         | 5/62 [00:02<00:20,  2.73it/s, loss=29.4, v_num=0]#015Epoch 0:  10%|▉         | 6/62 [00:02<00:20,  2.68it/s, loss=29.4, v_num=0]#015Epoch 0:  10%|▉         | 6/62 [00:02<00:20,  2.67it/s, loss=29.4, v_num=0]#015Epoch 0:  11%|█▏        | 7/62 [00:03<00:20,  2.63it/s, loss=29.4, v_num=0]#015Epoch 0:  11%|█▏        | 7/62 [00:03<00:20,  2.63it/s, loss=28, v_num=0]  #015Epoch 0:  13%|█▎        | 8/62 [00:03<00:20,  2.60it/s, loss=28, v_num=0]#015Epoch 0:  13%|█▎        | 8/62 [00:03<00:20,  2.60it/s, loss=27.3, v_num=0]#015Epoch 0:  15%|█▍        | 9/62 [00:03<00:20,  2.57it/s, loss=27.3, v_num=0]#015Epoch 0:  15%|█▍        | 9/62 [00:03<00:20,  2.57it/s, loss=27.5, v_num=0]#015Epoch 0:  16%|█▌        | 10/62 [00:04<00:20,  2.55it/s, loss=27.5, v_num=0]#015Epoch 0:  16%|█▌        | 10/62 [00:04<00:20,  2.55it/s, loss=26.2, v_num=0]#015Epoch 0:  18%|█▊        | 11/62 [00:04<00:20,  2.53it/s, loss=26.2, v_num=0]#015Epoch 0:  18%|█▊        | 11/62 [00:04<00:20,  2.53it/s, loss=25.4, v_num=0]#015Epoch 0:  19%|█▉        | 12/62 [00:05<00:19,  2.51it/s, loss=25.4, v_num=0]#015Epoch 0:  19%|█▉        | 12/62 [00:05<00:19,  2.51it/s, loss=25.3, v_num=0]#015Epoch 0:  21%|██        | 13/62 [00:05<00:19,  2.50it/s, loss=25.3, v_num=0]#015Epoch 0:  21%|██        | 13/62 [00:05<00:19,  2.50it/s, loss=24.7, v_num=0]#015Epoch 0:  23%|██▎       | 14/62 [00:06<00:19,  2.49it/s, loss=24.7, v_num=0]#015Epoch 0:  23%|██▎       | 14/62 [00:06<00:19,  2.49it/s, loss=24.1, v_num=0]#015Epoch 0:  24%|██▍       | 15/62 [00:06<00:18,  2.48it/s, loss=24.1, v_num=0]#015Epoch 0:  24%|██▍       | 15/62 [00:06<00:18,  2.48it/s, loss=23.5, v_num=0]#015Epoch 0:  26%|██▌       | 16/62 [00:06<00:18,  2.47it/s, loss=23.5, v_num=0]#015Epoch 0:  26%|██▌       | 16/62 [00:06<00:18,  2.47it/s, loss=22, v_num=0]  #015Epoch 0:  27%|██▋       | 17/62 [00:07<00:18,  2.46it/s, loss=22, v_num=0]#015Epoch 0:  27%|██▋       | 17/62 [00:07<00:18,  2.46it/s, loss=22.1, v_num=0]#015Epoch 0:  29%|██▉       | 18/62 [00:07<00:17,  2.46it/s, loss=22.1, v_num=0]#015Epoch 0:  29%|██▉       | 18/62 [00:07<00:17,  2.46it/s, loss=21.6, v_num=0]#015Epoch 0:  31%|███       | 19/62 [00:08<00:17,  2.45it/s, loss=21.6, v_num=0]#015Epoch 0:  31%|███       | 19/62 [00:08<00:17,  2.45it/s, loss=20.7, v_num=0]#015Epoch 0:  32%|███▏      | 20/62 [00:08<00:17,  2.45it/s, loss=20.7, v_num=0]#015Epoch 0:  32%|███▏      | 20/62 [00:08<00:17,  2.45it/s, loss=20.4, v_num=0]#015Epoch 0:  34%|███▍      | 21/62 [00:09<00:16,  2.44it/s, loss=20.4, v_num=0]#015Epoch 0:  34%|███▍      | 21/62 [00:09<00:16,  2.44it/s, loss=20.1, v_num=0]#015Epoch 0:  35%|███▌      | 22/62 [00:09<00:16,  2.44it/s, loss=20.1, v_num=0]#015Epoch 0:  35%|███▌      | 22/62 [00:09<00:16,  2.44it/s, loss=19.2, v_num=0]#015Epoch 0:  37%|███▋      | 23/62 [00:09<00:16,  2.44it/s, loss=19.2, v_num=0]#015Epoch 0:  37%|███▋      | 23/62 [00:09<00:16,  2.44it/s, loss=18.7, v_num=0]#015Epoch 0:  39%|███▊      | 24/62 [00:10<00:15,  2.43it/s, loss=18.7, v_num=0]#015Epoch 0:  39%|███▊      | 24/62 [00:10<00:15,  2.43it/s, loss=17.6, v_num=0]#015Epoch 0:  40%|████      | 25/62 [00:10<00:15,  2.43it/s, loss=17.6, v_num=0]#015Epoch 0:  40%|████      | 25/62 [00:10<00:15,  2.43it/s, loss=16.7, v_num=0]#015Epoch 0:  42%|████▏     | 26/62 [00:11<00:14,  2.43it/s, loss=16.7, v_num=0]#015Epoch 0:  42%|████▏     | 26/62 [00:11<00:14,  2.43it/s, loss=16, v_num=0]  #015Epoch 0:  44%|████▎     | 27/62 [00:11<00:14,  2.43it/s, loss=16, v_num=0]#015Epoch 0:  44%|████▎     | 27/62 [00:11<00:14,  2.43it/s, loss=15.9, v_num=0]#015Epoch 0:  45%|████▌     | 28/62 [00:11<00:14,  2.42it/s, loss=15.9, v_num=0]#015Epoch 0:  45%|████▌     | 28/62 [00:11<00:14,  2.42it/s, loss=15.1, v_num=0]#015Epoch 0:  47%|████▋     | 29/62 [00:12<00:13,  2.42it/s, loss=15.1, v_num=0]#015Epoch 0:  47%|████▋     | 29/62 [00:12<00:13,  2.42it/s, loss=14.6, v_num=0]#015Epoch 0:  48%|████▊     | 30/62 [00:12<00:13,  2.42it/s, loss=14.6, v_num=0]#015Epoch 0:  48%|████▊     | 30/62 [00:12<00:13,  2.42it/s, loss=14.9, v_num=0]#015Epoch 0:  50%|█████     | 31/62 [00:13<00:12,  2.41it/s, loss=14.9, v_num=0]#015Epoch 0:  50%|█████     | 31/62 [00:13<00:12,  2.41it/s, loss=14.9, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   0%|          | 0/31 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   3%|▎         | 1/31 [00:00<00:06,  4.94it/s]#033[A#015Epoch 0:  53%|█████▎    | 33/62 [00:13<00:11,  2.52it/s, loss=14.9, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating:   6%|▋         | 2/31 [00:00<00:04,  5.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  10%|▉         | 3/31 [00:00<00:04,  6.19it/s]#033[A#015Epoch 0:  56%|█████▋    | 35/62 [00:13<00:10,  2.61it/s, loss=14.9, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating:  13%|█▎        | 4/31 [00:00<00:04,  6.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  16%|█▌        | 5/31 [00:00<00:04,  6.49it/s]#033[A#015Epoch 0:  60%|█████▉    | 37/62 [00:14<00:09,  2.70it/s, loss=14.9, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating:  19%|█▉        | 6/31 [00:00<00:03,  6.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  23%|██▎       | 7/31 [00:01<00:03,  6.62it/s]#033[A#015Epoch 0:  63%|██████▎   | 39/62 [00:14<00:08,  2.78it/s, loss=14.9, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating:  26%|██▌       | 8/31 [00:01<00:03,  6.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  29%|██▉       | 9/31 [00:01<00:03,  6.65it/s]#033[A#015Epoch 0:  66%|██████▌   | 41/62 [00:14<00:07,  2.86it/s, loss=14.9, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating:  32%|███▏      | 10/31 [00:01<00:03,  6.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  35%|███▌      | 11/31 [00:01<00:02,  6.72it/s]#033[A#015Epoch 0:  69%|██████▉   | 43/62 [00:14<00:06,  2.94it/s, loss=14.9, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating:  39%|███▊      | 12/31 [00:01<00:02,  6.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  42%|████▏     | 13/31 [00:01<00:02,  6.75it/s]#033[A#015Epoch 0:  73%|███████▎  | 45/62 [00:15<00:05,  3.02it/s, loss=14.9, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating:  45%|████▌     | 14/31 [00:02<00:02,  6.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  48%|████▊     | 15/31 [00:02<00:02,  6.72it/s]#033[A#015Epoch 0:  76%|███████▌  | 47/62 [00:15<00:04,  3.09it/s, loss=14.9, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating:  52%|█████▏    | 16/31 [00:02<00:02,  6.74it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  55%|█████▍    | 17/31 [00:02<00:02,  6.76it/s]#033[A#015Epoch 0:  79%|███████▉  | 49/62 [00:15<00:04,  3.15it/s, loss=14.9, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating:  58%|█████▊    | 18/31 [00:02<00:01,  6.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  61%|██████▏   | 19/31 [00:02<00:01,  6.76it/s]#033[A#015Epoch 0:  82%|████████▏ | 51/62 [00:16<00:03,  3.22it/s, loss=14.9, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating:  65%|██████▍   | 20/31 [00:03<00:01,  6.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  68%|██████▊   | 21/31 [00:03<00:01,  6.70it/s]#033[A#015Epoch 0:  85%|████████▌ | 53/62 [00:16<00:02,  3.28it/s, loss=14.9, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating:  71%|███████   | 22/31 [00:03<00:01,  6.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  74%|███████▍  | 23/31 [00:03<00:01,  6.71it/s]#033[A#015Epoch 0:  89%|████████▊ | 55/62 [00:16<00:02,  3.34it/s, loss=14.9, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating:  77%|███████▋  | 24/31 [00:03<00:01,  6.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  81%|████████  | 25/31 [00:03<00:00,  6.71it/s]#033[A#015Epoch 0:  92%|█████████▏| 57/62 [00:17<00:01,  3.40it/s, loss=14.9, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating:  84%|████████▍ | 26/31 [00:03<00:00,  6.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  87%|████████▋ | 27/31 [00:04<00:00,  6.69it/s]#033[A#015Epoch 0:  95%|█████████▌| 59/62 [00:17<00:00,  3.46it/s, loss=14.9, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating:  90%|█████████ | 28/31 [00:04<00:00,  6.70it/s]#033[A\u001b[0m\n",
      "\n",
      "2022-02-04 14:05:21 Uploading - Uploading generated training model\u001b[34m#015Validating:  94%|█████████▎| 29/31 [00:04<00:00,  6.69it/s]#033[A#015Epoch 0:  98%|█████████▊| 61/62 [00:17<00:00,  3.51it/s, loss=14.9, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating:  97%|█████████▋| 30/31 [00:04<00:00,  6.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating: 100%|██████████| 31/31 [00:04<00:00,  6.72it/s]#033[A#015Epoch 0: 100%|██████████| 62/62 [00:17<00:00,  3.50it/s, loss=14.9, v_num=0, val_loss=13.10]\u001b[0m\n",
      "\u001b[34m#015                                                           #033[A#015Epoch 0: 100%|██████████| 62/62 [00:18<00:00,  3.47it/s, loss=14.9, v_num=0, val_loss=13.10]\u001b[0m\n",
      "\u001b[34mTraining complete.\u001b[0m\n",
      "\n",
      "2022-02-04 14:05:49 Completed - Training job completed\n",
      "ProfilerReport-1643983104: NoIssuesFound\n",
      "Training seconds: 276\n",
      "Billable seconds: 276\n",
      "------------------"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8199988a9fdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# deploy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ml.g4dn.2xlarge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m             \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         )\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\u001b[0m\n\u001b[1;32m   3568\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3570\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   3073\u001b[0m         )\n\u001b[1;32m   3074\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   3349\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDescribeEndpoint\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3350\u001b[0m         \"\"\"\n\u001b[0;32m-> 3351\u001b[0;31m         \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wait_until\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_deploy_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3352\u001b[0m         \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"EndpointStatus\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_wait_until\u001b[0;34m(callable_fn, poll)\u001b[0m\n\u001b[1;32m   4694\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4695\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4696\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4697\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4698\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "from sagemaker.estimator import Estimator\n",
    "import subprocess\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "instance_type = 'ml.p3.2xlarge'\n",
    "  \n",
    "print(\"Instance type = \" + instance_type)\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      base_job_name = 'asteroid',\n",
    "                      volume_size=50,\n",
    "                      image_uri='310850127430.dkr.ecr.us-east-1.amazonaws.com/asteroid:latest')# need to change image uri\n",
    "\n",
    "estimator.fit('s3://dns4/cv') # need to change the data path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1388d4c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-14824260d414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.g4dn.xlarge'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mserializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mJSONSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdeserializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mJSONDeserializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         )\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\u001b[0m\n\u001b[1;32m   3568\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3570\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   3073\u001b[0m         )\n\u001b[1;32m   3074\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   3349\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDescribeEndpoint\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3350\u001b[0m         \"\"\"\n\u001b[0;32m-> 3351\u001b[0;31m         \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wait_until\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_deploy_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3352\u001b[0m         \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"EndpointStatus\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_wait_until\u001b[0;34m(callable_fn, poll)\u001b[0m\n\u001b[1;32m   4694\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4695\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4696\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4697\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4698\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n"
     ]
    }
   ],
   "source": [
    "# deploy\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "import json\n",
    "import time\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "model_path='s3://sagemaker-us-east-1-310850127430/asteroid-2022-02-04-13-58-24-521/output/model.tar.gz' # 需要修改模型路径\n",
    "model_name='asteroid'+ time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "model = Model(\n",
    "    image_uri=\"310850127430.dkr.ecr.us-east-1.amazonaws.com/asteroid:latest\",\n",
    "    name=model_name,\n",
    "    role=role,\n",
    "    model_data=model_path\n",
    ")\n",
    "\n",
    "endpoint_name = f'endpoint-{model_name}-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "predictor = model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b337186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#调用endpoint\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from helper.utils import *\n",
    "from code.pred_utils import *\n",
    "import boto3\n",
    "\n",
    "endpoint_name = 'endpoint-asteroid2022-02-15-14-54-45-2022-02-15-14-54-45' #需要修改endpoint_name\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# 数据处理function\n",
    "def preprocess_wav(filename, fs = 16000):\n",
    "    x, fs = librosa.load(filename, sr =fs)\n",
    "    return x, fs\n",
    "\n",
    "\n",
    "def pad_list(xs, pad_value):\n",
    "    n_batch = len(xs)\n",
    "    max_len = max(x.size(0) for x in xs)\n",
    "    pad = xs[0].new(n_batch, max_len, * xs[0].size()[1:]).fill_(pad_value)\n",
    "    for i in range(n_batch):\n",
    "        pad[i, :xs[i].size(0)] = xs[i]\n",
    "    return pad\n",
    "\n",
    "def remove_pad(inputs, inputs_lengths):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inputs: torch.Tensor, [B, C, T] or [B, T], B is batch size\n",
    "        inputs_lengths: torch.Tensor, [B]\n",
    "    Returns:\n",
    "        results: a list containing B items, each item is [C, T], T varies\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    dim = inputs.dim()\n",
    "    if dim == 3:\n",
    "        C = inputs.size(1)\n",
    "    for input, length in zip(inputs, inputs_lengths):\n",
    "        if dim == 3: # [B, C, T]\n",
    "            results.append(input[:,:length].view(C, -1).cpu().numpy())\n",
    "        elif dim == 2:  # [B, T]\n",
    "            results.append(input[:length].view(-1).cpu().numpy())\n",
    "    return results\n",
    "\n",
    "\n",
    "def separate_process(xs):\n",
    "    # perform padding and convert to tensor\n",
    "    mixtures_pad = pad_list(xs, 0)\n",
    "    \n",
    "    mix_lengths = torch.tensor([len(item) for item in xs]) #torch.tensor([len(a) for a in xs])\n",
    "\n",
    "#     print(mixtures_pad.numpy().tolist())\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps({'data': mixtures_pad.numpy().tolist()}).encode('utf-8')\n",
    "    )\n",
    "    response=json.loads(response['Body'].read())['response']\n",
    "    response = torch.Tensor(response).cpu() \n",
    "    # Remove padding and flat\n",
    "    flat_estimate = remove_pad(response, mix_lengths)\n",
    "    return flat_estimate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7b284072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:23,  8.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 由于语音分离耗时较长，采用分割并行处理的形式\n",
    "filename = 'test.wav' #(10376000,)\n",
    "\n",
    "instance_time = 1\n",
    "\n",
    "x, fs = preprocess_wav(filename)\n",
    "s_x = instance_time * fs\n",
    "\n",
    "def batchify(input_np_arr, n = 1, frame_size = s_x): # 批量预测提速 默认n=1\n",
    "    batch = []\n",
    "    framed = librosa.util.frame(input_np_arr, frame_length=frame_size, hop_length=frame_size, axis=0)\n",
    "    for i, fr in enumerate(framed):\n",
    "        batch.append(torch.from_numpy(fr).float())\n",
    "        if (i+1) % n == 0:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if len(batch) > 0:\n",
    "        yield batch\n",
    "        \n",
    "s1 = []\n",
    "s2 = []\n",
    "for batch in tqdm(batchify(x)):\n",
    "    est = separate_process(batch)\n",
    "    for b in range(len(est)):\n",
    "        s1.extend([est[b][0]])\n",
    "        s2.extend([est[b][1]])\n",
    "\n",
    "s1 = np.concatenate(s1)\n",
    "s2 = np.concatenate(s2)\n",
    "s1 = s1 * 10\n",
    "s2 = s2 * 10\n",
    "write(s1, './complete_split_s1.wav')\n",
    "write(s2, './complete_split_s2.wav')\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8bd698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local train\n",
    "\n",
    "# import sagemaker\n",
    "# from sagemaker.local import LocalSession\n",
    "# from sagemaker.pytorch import PyTorch\n",
    "# import boto3\n",
    "# from sagemaker import get_execution_role\n",
    "\n",
    "# boto_session = boto3.Session(region_name='us-east-1')\n",
    "# sagemaker_session = LocalSession(boto_session=boto_session)\n",
    "# sagemaker_session.config = {'local': {'local_code': True}}\n",
    "# role = get_execution_role()\n",
    "\n",
    "# pytorch_estimator = PyTorch(entry_point='dprnn-train-deploy.py',\n",
    "#                             instance_type='local',\n",
    "#                             instance_count=1,\n",
    "#                             image_uri='310850127430.dkr.ecr.us-east-1.amazonaws.com/asteroid:latest',\n",
    "#                             sagemaker_session=sagemaker_session,\n",
    "#                             role=role,\n",
    "#                             source_dir='/home/ec2-user/SageMaker/asteroid_byoc/code')\n",
    "\n",
    "# pytorch_estimator.fit('s3://dns4/cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local test\n",
    "\n",
    "# import librosa\n",
    "# from tqdm import tqdm\n",
    "# from helper.utils import *\n",
    "# from code.pred_utils import *\n",
    "# import time\n",
    "# from tqdm import tqdm as tqdm\n",
    "# from asteroid.models import DPRNNTasNet\n",
    "\n",
    "# def model_fn(model_dir):\n",
    "#     model_path = os.path.join(model_dir, 'best_model.pth')\n",
    "#     model = DPRNNTasNet.from_pretrained(model_path)\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def predict_fn(input_data, model):\n",
    "#     # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     # model.to(device)\n",
    "#     model.eval()\n",
    "#     model.cuda()\n",
    "#     with torch.no_grad():\n",
    "#         estimate_source = model(input_data)  # [B, C, T]\n",
    "\n",
    "#     return estimate_source\n",
    "\n",
    "\n",
    "# def preprocess_wav(filename, fs = 16000):\n",
    "#     x, fs = librosa.load(filename, sr =fs)\n",
    "#     x_mirror = x\n",
    "#     asl_level = -26.0\n",
    "#     y = x + x_mirror\n",
    "# #     y = y/10**(asl_meter(y, fs)/20) * 10**(asl_level/20)\n",
    "#     return y, fs\n",
    "\n",
    "\n",
    "# def pad_list(xs, pad_value):\n",
    "#     n_batch = len(xs)\n",
    "#     max_len = max(x.size(0) for x in xs)\n",
    "#     pad = xs[0].new(n_batch, max_len, * xs[0].size()[1:]).fill_(pad_value)\n",
    "#     for i in range(n_batch):\n",
    "#         pad[i, :xs[i].size(0)] = xs[i]\n",
    "#     return pad\n",
    "\n",
    "# def remove_pad(inputs, inputs_lengths):\n",
    "#     \"\"\"\n",
    "#     Args:\n",
    "#         inputs: torch.Tensor, [B, C, T] or [B, T], B is batch size\n",
    "#         inputs_lengths: torch.Tensor, [B]\n",
    "#     Returns:\n",
    "#         results: a list containing B items, each item is [C, T], T varies\n",
    "#     \"\"\"\n",
    "#     results = []\n",
    "#     dim = inputs.dim()\n",
    "#     if dim == 3:\n",
    "#         C = inputs.size(1)\n",
    "#     for input, length in zip(inputs, inputs_lengths):\n",
    "#         if dim == 3: # [B, C, T]\n",
    "#             results.append(input[:,:length].view(C, -1).cpu().numpy())\n",
    "#         elif dim == 2:  # [B, T]\n",
    "#             results.append(input[:length].view(-1).cpu().numpy())\n",
    "#     return results\n",
    "\n",
    "# def separate_process(xs):\n",
    "#     # perform padding and convert to tensor\n",
    "#     mixtures_pad = pad_list(xs, 0).cuda()\n",
    "    \n",
    "#     mix_lengths = torch.tensor([len(item) for item in xs]) #torch.tensor([len(a) for a in xs])\n",
    "#     # Forward\n",
    "#     response = predict_fn(mixtures_pad, model)\n",
    "#     # Remove padding and flat\n",
    "#     flat_estimate = remove_pad(response, mix_lengths)\n",
    "#     return flat_estimate\n",
    "\n",
    "# def batchify(input_np_arr, n = 8, frame_size = s_x):\n",
    "#     batch = []\n",
    "#     framed = librosa.util.frame(input_np_arr, frame_length=frame_size, hop_length=frame_size, axis=0)\n",
    "#     for i, fr in enumerate(framed):\n",
    "#         batch.append(torch.from_numpy(fr).float())\n",
    "#         if (i+1) % n == 0:\n",
    "#             yield batch\n",
    "#             batch = []\n",
    "#     if len(batch) > 0:\n",
    "#         yield batch\n",
    "        \n",
    "# model = model_fn('./code/')\n",
    "\n",
    "# filename = 'test.wav' \n",
    "\n",
    "# instance_time = 1\n",
    "\n",
    "# x, fs = preprocess_wav(filename)\n",
    "# s_x = instance_time * fs\n",
    "\n",
    "# s1 = []\n",
    "# s2 = []\n",
    "\n",
    "# for batch in tqdm(batchify(x)):\n",
    "#     est = separate_process(batch)\n",
    "#     for b in range(len(est)):\n",
    "#         s1.extend([est[b][0]])\n",
    "#         s2.extend([est[b][1]])\n",
    "\n",
    "# s1 = np.concatenate(s1)\n",
    "# s2 = np.concatenate(s2)\n",
    "# s1 = s1 * 10\n",
    "# s2 = s2 * 10\n",
    "# write(s1, './complete_split_s1.wav')\n",
    "# write(s2, './complete_split_s2.wav')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c463df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# est=np.load('test.npy')\n",
    "# # response = client.invoke_endpoint(\n",
    "# #     EndpointName=endpoint_name,\n",
    "# #     ContentType=\"application/json\",\n",
    "# #     Body=json.dumps({'data': data.tolist()}).encode('utf-8')\n",
    "# # )\n",
    "# # local test\n",
    "# from io import BytesIO\n",
    "# import io\n",
    "# import base64\n",
    "# import json\n",
    "# import requests\n",
    "# dictionary = {'data':est.tolist()}\n",
    "# jsonString = json.dumps(dictionary, indent=4)\n",
    "# res = requests.post(\"http://localhost:8080/invocations\", data = jsonString) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e86e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = client.invoke_endpoint(\n",
    "#     EndpointName='endpoint-asteroid2022-02-15-14-54-45-2022-02-15-14-54-45',\n",
    "#     ContentType=\"application/json\",\n",
    "# #         Body=json.dumps({'data': mixtures_pad.numpy().tolist()}).encode('utf-8')\n",
    "#     Body=json.dumps({'data': est.tolist()}).encode('utf-8')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ebc9442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = response['Body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "822e780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(json.loads(test)['response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
